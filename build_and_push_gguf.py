import os
import subprocess
from huggingface_hub import snapshot_download, HfApi

def run_command(command):
    print(f"\n[Denglish-AI] Äang cháº¡y lá»‡nh: {command}")
    subprocess.run(command, shell=True, check=True)

def main():
    repo_id = "phgrouptechs/Denglish-8B-Instruct"
    
    # Äá»”I ÄÆ¯á»œNG DáºªN LÆ¯U GGUF SANG á»” /tmp Äá»‚ KHÃ”NG Bá»Š FULL á»” Máº NG
    gguf_f16_path = "/tmp/Denglish-8B-Instruct-F16.gguf"
    gguf_q4_path = "/tmp/Denglish-8B-Instruct-Q4_K_M.gguf"
    repo_filename = "Denglish-8B-Instruct-Q4_K_M.gguf" # TÃªn hiá»ƒn thá»‹ trÃªn web Hugging Face
    
    print("\n--- BÆ¯á»šC 1: Táº£i Model tá»« Hugging Face ---")
    model_path = snapshot_download(repo_id=repo_id)
    print(f"Model Ä‘Ã£ táº£i xong táº¡i: {model_path}")
    
    print("\n--- BÆ¯á»šC 2: CÃ i Ä‘áº·t & BiÃªn dá»‹ch llama.cpp báº±ng CMake ---")
    if not os.path.exists("llama.cpp"):
        run_command("git clone https://github.com/ggerganov/llama.cpp")
    
    if not os.path.exists("llama.cpp/build"):
        run_command("cd llama.cpp && pip install -r requirements.txt && cmake -B build && cmake --build build --config Release")
    else:
        print("ÄÃ£ biÃªn dá»‹ch llama.cpp xong, bá» qua bÆ°á»›c build.")

    print("\n--- BÆ¯á»šC 3: Chuyá»ƒn Ä‘á»•i sang GGUF (F16) ---")
    if not os.path.exists(gguf_f16_path):
        run_command(f"python llama.cpp/convert_hf_to_gguf.py {model_path} --outfile {gguf_f16_path} --outtype f16")
    else:
        print(f"File {gguf_f16_path} Ä‘Ã£ tá»“n táº¡i.")

    print("\n--- BÆ¯á»šC 4: LÆ°á»£ng tá»­ hÃ³a xuá»‘ng Q4_K_M ---")
    if not os.path.exists(gguf_q4_path):
        quantize_cmd = "./llama.cpp/build/bin/llama-quantize"
        if not os.path.exists(quantize_cmd):
            quantize_cmd = "./llama.cpp/build/llama-quantize"
            
        run_command(f"{quantize_cmd} {gguf_f16_path} {gguf_q4_path} q4_k_m")
    else:
        print(f"File {gguf_q4_path} Ä‘Ã£ tá»“n táº¡i.")

    print("\n--- BÆ¯á»šC 5: Äáº©y file GGUF lÃªn Hugging Face ---")
    api = HfApi()
    
    api.upload_file(
        path_or_fileobj=gguf_q4_path,
        path_in_repo=repo_filename,
        repo_id=repo_id,
        repo_type="model",
        commit_message="Add Q4_K_M GGUF model for CPU inference"
    )
    
    print("\nğŸ‰ HOÃ€N Táº¤T! ÄÃ£ Ä‘áº©y file 4-bit lÃªn Hugging Face.")
    
    # Dá»n dáº¹p file 16GB trung gian Ä‘á»ƒ trÃ¡nh náº·ng mÃ¡y
    if os.path.exists(gguf_f16_path):
        os.remove(gguf_f16_path)
        print("ÄÃ£ dá»n dáº¹p file táº¡m F16.")

if __name__ == "__main__":
    main()