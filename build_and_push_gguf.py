import os
import subprocess
from huggingface_hub import snapshot_download, HfApi

def run_command(command):
    """HÃ m há»— trá»£ cháº¡y lá»‡nh terminal tá»« Python"""
    print(f"\n[Denglish-AI] Äang cháº¡y lá»‡nh: {command}")
    subprocess.run(command, shell=True, check=True)

def main():
    # Cáº¥u hÃ¬nh Repo vÃ  TÃªn file
    repo_id = "phgrouptechs/Denglish-8B-Instruct"
    gguf_f16_name = "Denglish-8B-Instruct-F16.gguf"
    gguf_q4_name = "Denglish-8B-Instruct-Q4_K_M.gguf"
    
    # 1. Táº£i báº£n gá»‘c (Base weights) tá»« Hugging Face vá» mÃ¡y (Cache)
    print("\n--- BÆ¯á»šC 1: Táº£i Model tá»« Hugging Face ---")
    model_path = snapshot_download(repo_id=repo_id)
    print(f"Model Ä‘Ã£ táº£i xong táº¡i: {model_path}")
    
    # 2. Chuáº©n bá»‹ cÃ´ng cá»¥ llama.cpp
    print("\n--- BÆ¯á»šC 2: CÃ i Ä‘áº·t & BiÃªn dá»‹ch llama.cpp ---")
    if not os.path.exists("llama.cpp"):
        run_command("git clone https://github.com/ggerganov/llama.cpp")
        # BiÃªn dá»‹ch llama.cpp Ä‘á»ƒ láº¥y cÃ´ng cá»¥ llama-quantize
        run_command("cd llama.cpp && pip install -r requirements.txt && make")
    else:
        print("ThÆ° má»¥c llama.cpp Ä‘Ã£ tá»“n táº¡i, bá» qua bÆ°á»›c táº£i láº¡i.")

    # 3. Chuyá»ƒn Ä‘á»•i sang chuáº©n GGUF (16-bit)
    print("\n--- BÆ¯á»šC 3: Chuyá»ƒn Ä‘á»•i sang GGUF (F16) ---")
    if not os.path.exists(gguf_f16_name):
        run_command(f"python llama.cpp/convert_hf_to_gguf.py {model_path} --outfile {gguf_f16_name} --outtype f16")
    else:
        print(f"File {gguf_f16_name} Ä‘Ã£ tá»“n táº¡i.")

    # 4. Ã‰p xung (Quantize) xuá»‘ng 4-bit (SiÃªu nháº¹, dÃ nh cho CPU)
    print("\n--- BÆ¯á»šC 4: LÆ°á»£ng tá»­ hÃ³a xuá»‘ng Q4_K_M ---")
    if not os.path.exists(gguf_q4_name):
        run_command(f"./llama.cpp/llama-quantize {gguf_f16_name} {gguf_q4_name} q4_k_m")
    else:
        print(f"File {gguf_q4_name} Ä‘Ã£ tá»“n táº¡i.")

    # 5. Upload file GGUF 4-bit lÃªn láº¡i Hugging Face
    print("\n--- BÆ¯á»šC 5: Äáº©y file GGUF lÃªn Hugging Face ---")
    api = HfApi()
    
    # Äáº©y file 4-bit (File quan trá»ng nháº¥t cho CPU)
    api.upload_file(
        path_or_fileobj=gguf_q4_name,
        path_in_repo=gguf_q4_name, # TÃªn hiá»ƒn thá»‹ trÃªn Hugging Face
        repo_id=repo_id,
        repo_type="model",
        commit_message="Add Q4_K_M GGUF model for CPU inference"
    )
    
    print("\nğŸ‰ HOÃ€N Táº¤T! Báº¡n cÃ³ thá»ƒ kiá»ƒm tra kho Hugging Face cá»§a mÃ¬nh.")

if __name__ == "__main__":
    main()