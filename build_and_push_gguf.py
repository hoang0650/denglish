import os
import subprocess
from huggingface_hub import snapshot_download, HfApi

def run_command(command):
    print(f"\n[Denglish-AI] ƒêang ch·∫°y l·ªánh: {command}")
    subprocess.run(command, shell=True, check=True)

def main():
    repo_id = "phgrouptechs/Denglish-8B-Instruct"
    gguf_f16_name = "Denglish-8B-Instruct-F16.gguf"
    gguf_q4_name = "Denglish-8B-Instruct-Q4_K_M.gguf"
    
    print("\n--- B∆Ø·ªöC 1: T·∫£i Model t·ª´ Hugging Face ---")
    model_path = snapshot_download(repo_id=repo_id)
    print(f"Model ƒë√£ t·∫£i xong t·∫°i: {model_path}")
    
    print("\n--- B∆Ø·ªöC 2: C√†i ƒë·∫∑t & Bi√™n d·ªãch llama.cpp b·∫±ng CMake ---")
    if not os.path.exists("llama.cpp"):
        run_command("git clone https://github.com/ggerganov/llama.cpp")
    
    # S·ª≠ d·ª•ng CMake thay cho make theo b·∫£n c·∫≠p nh·∫≠t m·ªõi nh·∫•t c·ªßa llama.cpp
    if not os.path.exists("llama.cpp/build"):
        run_command("cd llama.cpp && pip install -r requirements.txt && cmake -B build && cmake --build build --config Release")
    else:
        print("ƒê√£ bi√™n d·ªãch llama.cpp xong, b·ªè qua b∆∞·ªõc build.")

    print("\n--- B∆Ø·ªöC 3: Chuy·ªÉn ƒë·ªïi sang GGUF (F16) ---")
    if not os.path.exists(gguf_f16_name):
        run_command(f"python llama.cpp/convert_hf_to_gguf.py {model_path} --outfile {gguf_f16_name} --outtype f16")
    else:
        print(f"File {gguf_f16_name} ƒë√£ t·ªìn t·∫°i.")

    print("\n--- B∆Ø·ªöC 4: L∆∞·ª£ng t·ª≠ h√≥a xu·ªëng Q4_K_M ---")
    if not os.path.exists(gguf_q4_name):
        # ƒê∆∞·ªùng d·∫´n file l∆∞·ª£ng t·ª≠ h√≥a sau khi build b·∫±ng CMake th∆∞·ªùng n·∫±m ·ªü 1 trong 2 v·ªã tr√≠ n√†y
        quantize_cmd = "./llama.cpp/build/bin/llama-quantize"
        if not os.path.exists(quantize_cmd):
            quantize_cmd = "./llama.cpp/build/llama-quantize"
            
        run_command(f"{quantize_cmd} {gguf_f16_name} {gguf_q4_name} q4_k_m")
    else:
        print(f"File {gguf_q4_name} ƒë√£ t·ªìn t·∫°i.")

    print("\n--- B∆Ø·ªöC 5: ƒê·∫©y file GGUF l√™n Hugging Face ---")
    api = HfApi()
    
    api.upload_file(
        path_or_fileobj=gguf_q4_name,
        path_in_repo=gguf_q4_name,
        repo_id=repo_id,
        repo_type="model",
        commit_message="Add Q4_K_M GGUF model for CPU inference (Built with CMake)"
    )
    
    print("\nüéâ HO√ÄN T·∫§T! ƒê√£ ƒë·∫©y file 4-bit l√™n Hugging Face.")

if __name__ == "__main__":
    main()